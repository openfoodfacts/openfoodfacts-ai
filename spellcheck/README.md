# Spellcheck

## Guidelines

The influence of the Spellcheck on the list of ingredients needs to be controlled to avoid alterate contributions and/or add new errors. Therefore, we keep the modification to a minimum to favour Precision over Recall.

From the different types of errors observed across products, we came with these spellcheck guidelines:

* Correct typos in ingredient words if word is recognized;
* Whitespaces between words and percentages shouldn't be corrected. The text needs to be kept as unchanged as possible.
(Example: `Ingredient 0,2   %`);
* Some ingredients are enclosed with `_`, such as `_milk_` or `_cacahuetes_`, to detect allergens. Needs to be unchanged by the spellcheck. However, in the case it is not an ingredient, such as `_Cacahuetes_ con cáscara tostado. _Trazas de frutos de cáscara_.`, it needs to be modified into `_Cacahuetes_ con cáscara tostado. Trazas de frutos de cáscara.`;
* Some percentages were badly parsed by the OCR, such as `cheese (196)` instead of `cheese (1%)` or `καραμέλα (396` instead of `καραμέλα (3%)` . Since there is a recognizable pattern, `%` being transformed into `96`, the spellcheck should be able to recognize and correct it.

## Benchmark - Validation dataset

To improve the quality of the Spellcheck module, we decided to exploit the recent advancements with LLMs to train a task-specific Machine Learning model on OFF data. 

Creating this kind of solution requires rigorously building a benchmark/validation dataset to estimate the future models' performances. 

Our idea is to use the existing dataset developed a few years ago, enhance it with new data, and then perform data augmentation using LLMs.

Not only do we build a benchmark to evaluate future solutions, but we'll also use data augmentation to create the dataset required to train a task-specific machine learning model.

### Data lineage

*Data*
```bash
├── data
│   ├── benchmark
│   │   ├── benchmark.json
│   │   ├── test_benchmark.json
│   │   └── verified_benchmark.parquet
│   ├── fr
│   │   ├── 0_fr_data.json
│   │   └── 1_old_fr_no_duplicate_data.json
│   └── labeled
│       └── corrected_list_of_ingredients.txt
```

*Scripts*
```bash
├── scripts
│   ├── argilla
│   │   ├── benchmark.py
│   │   └── extract_benchmark.py
│   ├── benchmark
│   │   ├── create_benchmark.py
│   │   ├── create_test_benchmark.py
│   │   └── evaluation.py
│   └── old_to_new
│       ├── 0_convert_old_data.py
│       └── 1_old_fr_data_check.ipynb
```

The benchmark is composed of **247** lists of ingredients from 3 data sources:

* **30%** of the old dataset composed of manually corrected lists of ingredients in French from the previous work by Lucain W. The old dataset, `spellcheck/old/test_sets/fr` is used  to constitute our new dataset. It is composed of `List of Ingredients` before and after spellcheck, mainly in French. The processing scripts are located at: `spellcheck/scripts/old_to_new` - and the processed data in `spellcheck/data/fr`.

    * `data/fr/0_fr_data.json`: the old data is extracted and transformed into a json file. Basic processing are performed, such as removing *NO_VALID* data (data size: **786**) - script: `scripts/old_to_new/0_convert_old_data.py`

    * `data/1_old_fr_no_duplicate_data.json`: We noticed a lot of duplicates before and after spellcheck, representing almost half of the dataset. We remove them (data size: **441**) - script: `scripts/old_to_new/1_old_fr_data_check.ipynb`

* **15** manually corrected lists of ingredients in different languages. 
    
    * This small sample was used to prompt engineer GPT-3.5 on achieving good performances on the spellcheck task.
    * Those examples mainly comes from the OFF website.
    * data: `data/labeled/corrected_list_of_ingredients.txt` & `data/benchmark/test_benchmark.json` - script: `scripts/benchmark/create_test_benchmark.py` 

* **100** lists of ingredients with the tag `50-percent-unknown` corrected with the prompted GPT-3.5. It follows the correction guidelines defined with the OFF team and based on observations in production. You'll find the prompt used to augmente the data with GPT-3.5 at `utils/prompt.py`. These 100 lists of ingredients are extracted from the OFF database and processed right away during the benchmark creation.

Benchmark composition script: `scripts/benchmark/create_benchmark.py`

Once composed, the benchmark is then verified  using Argilla to ensure the correction generated by OpenAI respect the Spellcheck guidelines. The corrected benchmark is located at `data/benchmark/verified_benchmark.parquet`.

### Argilla

Argilla is an open-source annotation tool specific to Natural Language Processing.

To annotate and verify the benchmark, we deployed an Argilla instance and manually verified the correction generated by GPT-3.5.

Scripts:
* `scripts/argilla/benchmark.py`: structure of the annotation tool for the spellcheck task
* `scripts/argilla/extract_benchmark.py`: script to extract the annotated dataset from Argilla. The extracted dataset is saved at `data/benchmark/verified_benchmark.parquet`.
