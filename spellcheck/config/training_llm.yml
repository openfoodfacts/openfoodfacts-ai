estimator:
  model: "mistral-7b-instruct-v3"
  entry_point: "llm.py"
  source_dir: "scripts/training/llm"
  dependencies: 
    - "src/"
  output_path: "s3://open-food-facts-robotoff/spellcheck/model-training/"
  code_location: "s3://open-food-facts-robotoff/spellcheck/model-training/"
  base_job_name: "mistral-7b-instruct-v3"
  instance_count: 1
  instance_type: "ml.g5.2xlarge"
  transformers_version: "4.36"                         # the transformers version used in the training job
  pytorch_version: "2.1"                            # the pytorch_version version used in the training job
  py_version: "py310"                                  # the python version used in the training job
  disable_output_compression: true
  volume_size: 300
  comet_ml_tags: 
    - "mistral-7b-instruct-v0.3"
    # - "test"
    - "eval_loss"
    - "eval_normalization"
    - guidelines-instruction
  s3_evaluation_uri: "s3://open-food-facts-robotoff/spellcheck/evaluation_output/"

hyperparameters:
  training_data: "openfoodfacts/spellcheck-dataset"
  evaluation_data: "openfoodfacts/spellcheck-benchmark"
  training_data_version: v5.0
  evaluation_data_version: v7.4
  pretrained_model_name: "mistralai/Mistral-7B-Instruct-v0.3" 
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  lr: 0.0002                                    # Paper https://arxiv.org/pdf/2210.11416
  warmup_steps: 0
  warmup_ratio: 0.3
  weight_decay: 0.1
  gradient_checkpointing: true
  seed: 42
  optim: "adamw_torch_fused"                             # The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.
  lr_scheduler_type: "constant"
  gradient_accumulation_steps: 4
  bf16: true
  tf32: true
  fp16: false
  quantize: true
  logging_steps : 10
  eval_steps: 25
  save_total_limit: 1
  train_data_revision: "v5"
  