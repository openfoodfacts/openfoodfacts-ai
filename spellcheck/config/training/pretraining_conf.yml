estimator: 
  entry_point: "pretraining_llm.py"                                              # train script  
  source_dir: "scripts/training/llm/"                                           # directory containing training script and requirements requirements.
  dependencies: 
    - "src/"                                                                    # Additional local library
  output_path: "s3://open-food-facts-robotoff/spellcheck/model-training/"       # s3 path to save the artifacts
  code_location: "s3://open-food-facts-robotoff/spellcheck/model-training/"     # s3 path to stage the code during the training job
  base_job_name: "mistral-7b-v03"                                               # name of the training job
  instance_count: 1                                                             # the number of instances used for training
  instance_type: "ml.g5.2xlarge"                                                # instances type used for the training job
  transformers_version: "4.36"                                                  # transformers version used in the training job
  pytorch_version: "2.1"                                                        # pytorch_version version used in the training job
  py_version: "py310"                                                           # python version used in the training job
  disable_output_compression: true                                              # not compress output to save training time and cost
  volume_size: 300                                                              # the size of the EBS volume in GB
  
hyperparameters:
  # Data
  training_data: "openfoodfacts/spellcheck-corpus"
  train_split: "train"

  # Trainer
  output_dir: "/opt/ml/model"
  pretrained_model_name: "mistralai/Mistral-7B-v0.3" 
  num_train_epochs: 1
  per_device_train_batch_size: 4
  learning_rate: 0.0002                                            # Paper https://arxiv.org/pdf/2210.11416
  warmup_steps: 0
  warmup_ratio: 0.1
  weight_decay: 0.1
  gradient_checkpointing: true
  seed: 42
  optim: "adamw_torch_fused"                             # The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.
  lr_scheduler_type: "cosine"
  gradient_accumulation_steps: 8
  bf16: true
  tf32: true
  fp16: false
  logging_steps : 1
  save_total_limit: 1
  report_to: "none" # Important to avoid superposition of Trainer callback and our custom callback
  max_seq_length: 2048
  packing: true
  dataset_text_field: "ingredients_text"
  # add_special_tokens: true  # Add bos token and other special token from the tokenizer
  # append_concat_token: true  # If true, appends eos_token_id at the end of each sample being packed.

  # Saving
  merge_weights: true
  max_shard_size: "2GB"